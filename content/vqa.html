<!DOCTYPE html>
<html>
	<head>
		<title>VQA</title>
		<script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<link rel="stylesheet" type="text/css" href="../style/mystyle.css">
	</head>
	<body>
		<h1>Visual Question Answering</h1>
		<h2>Introduction</h2>
		<p>
			The VQA is a challenge to provide an accurate natural language answer to a given image and a natural language question. The questions are such that they target different areas of an image, including background details and underlying context. The VQA dataset consists of \(254721\) images (MSCOCO) and \(3\) questions per image. Also, 10 ground truth answers per question are provided out of which we choose only the most likely answer during training. We used Python and Keras for the implemetation. The code is hosted on GitHub. 
		</p>
		<h2> The Models</h2>
		<p>
			Our first model is the VIS+LSTM model introduced in the paper Exploring <a href="https://arxiv.org/abs/1505.02074">Models and Data for Image Question Answering</a>. In this model, the input question is tokenized and it's each word is mapped to a \(300\) length vector using the <a href="http://nlp.stanford.edu/projects/glove/">common crawl glove embeddings</a>. So, each sentence is a sequence of \(300\) length vectors. To make the length of all these sequences equal, we left pad with zero vectors. When Convolutional Neural Networks are trained on object recognition, they develop a representation of the image that makes object information increasingly explicit along the processing hierarchy. Therefore, along the processing hierarchy of the network, the input image is transformed into representations that increasingly care about the actual content of the image compared to it's detailed pixel values. To exploit this fact, the input image is passed through the VGG16 model and the output of it's last layer, a \(4096\) length vector, is taken. This \(4096\) length vector is transformed into a \(300\) length vector using an affine transform (a dense layer with no activation) and then concatenated at the beginning of the question sequence. The resulting sequence is sent to an LSTM model. Our LSTM model consists of an LSTM layer and dense layer with softmax activation. 		
		</p>
		<center>
		<figure>
			<img src="https://raw.githubusercontent.com/Deep-Learning-Projects/VQA/master/examples/model1.png">
			<figcaption>Fig-1: VIS+LSTM Model</figcaption>
		</figure>
		</center>
		<p>
			In the second model, there are two image feature inputs, one at the start of the sequence and one at the end of the sequence, with different learned linear transformations. 
		</p>
		<center>
		<figure>
			<img src="https://raw.githubusercontent.com/Deep-Learning-Projects/VQA/master/examples/model2.png">
			<figcaption>Fig-2: 2-VIS+LSTM Model</figcaption>
		</figure>
		</center>
	</body>
</html>
